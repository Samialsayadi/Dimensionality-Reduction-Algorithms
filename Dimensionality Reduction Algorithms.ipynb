{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sami\n"
     ]
    }
   ],
   "source": [
    "print('Sami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(r\"aji-Arabic_corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):\n",
    "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
    "    return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(r\"aji-Arabic_corpus.csv\")\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Corpus['text'],Corpus['targe'],test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 45642 features\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test=TFIDF(X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1200)\n",
    "X_train_new = svd.fit(X_train)\n",
    "X_train_new =  svd.transform(X_train)\n",
    "X_test_new = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with old features:  (1200, 45642)\n",
      "train with new features: (1200, 1200)\n",
      "test with old features:  (300, 45642)\n",
      "test with new features: [[ 0.21907775 -0.03349917 -0.08981092 ... -0.00185893 -0.00908093\n",
      "  -0.0074321 ]\n",
      " [ 0.19894801 -0.00249395 -0.04212932 ...  0.00717913  0.00451206\n",
      "   0.01065089]\n",
      " [ 0.17899972 -0.02885518 -0.05548208 ...  0.00381085  0.00165989\n",
      "  -0.00867605]\n",
      " ...\n",
      " [ 0.16544857 -0.07868928  0.08791505 ...  0.00257164  0.00108691\n",
      "   0.00340264]\n",
      " [ 0.29541793  0.42333497  0.14022248 ...  0.00301589 -0.00319939\n",
      "   0.00968252]\n",
      " [ 0.22670496 -0.0188487  -0.07174499 ...  0.00129422  0.00290858\n",
      "   0.00277585]]\n"
     ]
    }
   ],
   "source": [
    "print(\"train with old features: \",np.array(X_train).shape)\n",
    "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
    "\n",
    "print(\"test with old features: \",np.array(X_test).shape)\n",
    "print(\"test with new features:\" ,np.array(X_test_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "NMF_ = NMF(n_components=2000)\n",
    "X_train_new = NMF_.fit(X_train)\n",
    "X_train_new =  NMF_.transform(X_train)\n",
    "X_test_new = NMF_.transform(X_test)\n",
    "\n",
    "print(\"train with old features: \",np.array(X_train).shape)\n",
    "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
    "\n",
    "print(\"test with old features: \",np.array(X_test).shape)\n",
    "print(\"test with new features:\" ,np.array(X_test_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Projection: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random feature is a dimensionality reduction technique mostly used for very large volume dataset or very high dimensional feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with old features:  (1200, 45987)\n",
      "train with new features: (1200, 2000)\n",
      "test with old features:  (300, 45987)\n",
      "test with new features: (300, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import random_projection\n",
    "\n",
    "RandomProjection = random_projection.GaussianRandomProjection(n_components=2000)\n",
    "X_train_new = RandomProjection.fit_transform(X_train)\n",
    "X_test_new = RandomProjection.transform(X_test)\n",
    "\n",
    "print(\"train with old features: \",np.array(X_train).shape)\n",
    "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
    "\n",
    "print(\"test with old features: \",np.array(X_test).shape)\n",
    "print(\"test with new features:\" ,np.array(X_test_new).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with old features:  (1200, 45987)\n",
      "train with new features: (1200, 1200)\n",
      "test with old features:  (300, 45987)\n",
      "test with new features: (300, 1200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1200)\n",
    "X_train_new = pca.fit_transform(X_train)\n",
    "X_test_new = pca.transform(X_test)\n",
    "\n",
    "print(\"train with old features: \",np.array(X_train).shape)\n",
    "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
    "\n",
    "print(\"test with old features: \",np.array(X_test).shape)\n",
    "print(\"test with new features:\" ,np.array(X_test_new).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with old features:  (1200, 45987)\n",
      "train with new features: (1200, 4)\n",
      "test with old features:  (300, 45987)\n",
      "test with new features: (300, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "X_train_new = LDA.fit(X_train,y_train)\n",
    "X_train_new =  LDA.transform(X_train)\n",
    "X_test_new = LDA.transform(X_test)\n",
    "\n",
    "print(\"train with old features: \",np.array(X_train).shape)\n",
    "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
    "\n",
    "print(\"test with old features: \",np.array(X_test).shape)\n",
    "print(\"test with new features:\" ,np.array(X_test_new).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random_projection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with old features:  (1200, 45987)\n",
      "train with new features: (1200, 2000)\n",
      "test with old features:  (300, 45987)\n",
      "test with new features: (300, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import random_projection\n",
    "\n",
    "RandomProjection = random_projection.GaussianRandomProjection(n_components=2000)\n",
    "X_train_new = RandomProjection.fit_transform(X_train)\n",
    "X_test_new = RandomProjection.transform(X_test)\n",
    "\n",
    "print(\"train with old features: \",np.array(X_train).shape)\n",
    "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
    "\n",
    "print(\"test with old features: \",np.array(X_test).shape)\n",
    "print(\"test with new features:\" ,np.array(X_test_new).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 1500\n",
    "n=1500\n",
    "# this is our input placeholder\n",
    "input = Input(shape=(n,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(n, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input, encoded)\n",
    "\n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.astype('float32') / 255.\n",
    "x_test = y_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #X_train, X_test, y_train, y_test\n",
    "\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(y_train, y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
